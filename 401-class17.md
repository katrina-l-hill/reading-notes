# Read: Class 17

## Web Scrape with Python in 4 minutes

- Web scraping is a way to access large amounts of information from a website.  
- To web scrape information, you need to go to a website and extract the information from it.  
- The process is automatic and allows you to save time and effort in the process of obtaining the information.  
- To ensure you are legally obtaining the information from the website, you must ensure you read the Terms and Conditions, particularly if you’re looking to use the information for commercial purposes.  
- You also need to ensure you’re not downloading the data too fast as it may break the website you’re extracting data, which may cause you to be blocked from accessing it.  
- You need to inspect the website through the browser to identify where the data is located that you want to download. This information may be buried within several levels.  
- Once you find the location fo the data you want to extract, you need to import applicable libraries.  
- Once you’ve imported the libraries, you need to set the website’s URL and access it through the imported libraries.  
- A successful access to the website will output a code 200 response.  
- Next, you need to parse the information to get to the data link you want then extract the link.  
- It’s important to pause the extraction process so that the website doesn’t consider your requests as span.  
- Pausing uses the code “time.sleep(1).”  

## Web scraping

- Web scraping is also called “web harvesting” or “web data extraction.”  
- Web scraping software uses HTTP to access the internet.  
- The purpose of web scraping is typically to use the information extracted for another purpose.  
- The automated part of web scraping refers to using a bot or a web crawler to do the work for you instead of having to do it manually.  
- The bot or web crawler fetches, or downloads, a web page which the information is then extracted from it.  
- The information can then be “parsed, searched, formatted,” or put into a spreadsheet.  
- Web scraping can be done using the following techniques: human copy-and-paste, text pattern-matching, HTTP programming, HTML parsing, DOM parsing, vertical aggregation, semantic annotation recognizing, and computer vision web-page analysis.  
- It’s important to know that the legality of web scraping various around the world, so you have to be cognizant of the laws of each country you want to obtain data from within.  
- There are various ways to block web scraping, which include blocking an IP address, commercial anti-bot services, adding small variations in HTML and CSS that surround the data, and loading data directly into the HTML DOM.  

## How to scrape websites without getting blocked

- The basic rule for web scraping is to “be nice” and follow the website’s guidelines for scaping their data.  
- There are best practices to use when web scraping. These include respect robots.txt, don’t follow the same crawling pattern, use headless browsers, beware of honey pot traps, and use captcha solving services.  
- The robots.txt file is usually located in the root directory of a website and has specific rules for good behavior, so it’s beneficial to follow the guidelines inside the file.  
- Humans and bots behave differently, which is why some bots get caught web scraping and humans don’t. Bots tend to follow the routes they’ve been programmed with, so their behavior is predictable and trackable. If a website has anti-crawling software, bots will be discovered because of their repetitive actions.  
- Websites will check a browser to see if it’s real or being controlled by an automation library. The anti-scraping software being used to check the browser is looking to see if the browser can execute JavaScript code. To get around this, headless browser-based scrapers have been created to execute JavaScript code.  
- Honey pot traps are designed to lure hackers or web scrapers in to a website to gain information from them. This is done through invisible links that humans can’t see but web scrapers can.  
- Websites will use captcha pages to put up restrictions to accessing the data. To get past the captcha pages, you can use a captcha service.  

## Video – Track Amazon Prices

## Bookmark and Review – Beautiful Soup

- Beautiful Soup is a Python library that can be used for web-scraping.  
- It provides a toolkit for dissecting and extracting information from a document.  
- It automatically converts incoming documents to Unicode and outgoing documents to UTF-8.  
- It sits on top of Python parsers (e.g., lxml and html5lib) so you can try out different parsing strategies, for speed versus flexibility.  

## Things I want to know more about
